use anyhow::Result;
use rmcp::{model::*, Error as McpError};
use std::borrow::Cow;
use std::collections::HashMap;
use std::fs;
use std::io::Read;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::Duration;

use reqwest::header::{AUTHORIZATION, CONTENT_TYPE};
use reqwest::Client;
use ring::digest::{Context as ShaContext, SHA256};
use ignore::gitignore::{Gitignore, GitignoreBuilder};
use serde::{Deserialize, Serialize};
use encoding_rs::{GBK, WINDOWS_1252, UTF_8};
use globset::{Glob, GlobSet, GlobSetBuilder};

use super::types::{AcemcpRequest, AcemcpConfig};
use crate::log_debug;
use crate::log_important;

/// Acemcpå·¥å…·å®ç°
pub struct AcemcpTool;

impl AcemcpTool {
    /// æ‰§è¡Œä»£ç åº“æœç´¢
    pub async fn search_context(request: AcemcpRequest) -> Result<CallToolResult, McpError> {
        log_important!(info,
            "Acemcpæœç´¢è¯·æ±‚: project_root_path={}, query={}",
            request.project_root_path, request.query
        );

        // è¯»å–é…ç½®
        let mut acemcp_config = Self::get_acemcp_config()
            .await
            .map_err(|e| McpError::internal_error(format!("è·å–acemcpé…ç½®å¤±è´¥: {}", e), None))?;

        // è§„èŒƒåŒ– base_urlï¼ˆç¼ºåè®®æ—¶è¡¥ http://ï¼‰ï¼Œå¹¶å»é™¤æœ«å°¾æ–œæ 
        if let Some(base) = &acemcp_config.base_url {
            let normalized = normalize_base_url(base);
            acemcp_config.base_url = Some(normalized);
        }

        // æ‰§è¡Œï¼šå¢é‡ç´¢å¼•ï¼ˆå«æ‰¹é‡ä¸Šä¼ ï¼‰+ æ£€ç´¢
        match index_and_search(&acemcp_config, &request.project_root_path, &request.query).await {
            Ok(text) => Ok(CallToolResult { content: vec![Content::text(text)], is_error: None }),
            Err(e) => Ok(CallToolResult { content: vec![Content::text(format!("Acemcpæ‰§è¡Œå¤±è´¥: {}", e))], is_error: Some(true) })
        }
    }

    /// è·å–acemcpé…ç½®
    async fn get_acemcp_config() -> Result<AcemcpConfig> {
        // ä»é…ç½®æ–‡ä»¶ä¸­è¯»å–acemcpé…ç½®
        let config = crate::config::load_standalone_config()
            .map_err(|e| anyhow::anyhow!("è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {}", e))?;
        
        Ok(AcemcpConfig {
            base_url: config.mcp_config.acemcp_base_url,
            token: config.mcp_config.acemcp_token,
            batch_size: config.mcp_config.acemcp_batch_size,
            max_lines_per_blob: config.mcp_config.acemcp_max_lines_per_blob,
            text_extensions: config.mcp_config.acemcp_text_extensions,
            exclude_patterns: config.mcp_config.acemcp_exclude_patterns,
        })
    }

    /// è·å–å·¥å…·å®šä¹‰
    pub fn get_tool_definition() -> Tool {
        let schema = serde_json::json!({
            "type": "object",
            "properties": {
                "project_root_path": {
                    "type": "string",
                    "description": "ä½œæˆ˜åŸºåœ°çš„ç»å¯¹è·¯å¾„ï¼Œä½¿ç”¨æ­£æ–œæ (/)ä½œä¸ºåˆ†éš”ç¬¦ã€‚ä¾‹å¦‚ï¼šC:/Users/username/projects/myproject"
                },
                "query": {
                    "type": "string",
                    "description": "ç”¨äºæœç´¢ç›¸å…³ä»£ç æƒ…æŠ¥çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤ã€‚å¨éœ‡å¤©å°†æ‰§è¡Œè¯­ä¹‰æœç´¢å¹¶è¿”å›ä¸ç›®æ ‡åŒ¹é…çš„ä»£ç ç‰‡æ®µã€‚ä¾‹å¦‚ï¼š'æ—¥å¿—é…ç½®è®¾ç½®åˆå§‹åŒ–logger'ï¼ˆæœç´¢æ—¥å¿—è®¾ç½®ä»£ç ï¼‰ã€'ç”¨æˆ·è®¤è¯ç™»å½•'ï¼ˆæœç´¢è®¤è¯ç›¸å…³ä»£ç ï¼‰ã€'æ•°æ®åº“è¿æ¥æ± 'ï¼ˆæœç´¢æ•°æ®åº“è¿æ¥ä»£ç ï¼‰ã€‚è¿”å›å¸¦æœ‰æ–‡ä»¶è·¯å¾„å’Œè¡Œå·çš„æ ¼å¼åŒ–æƒ…æŠ¥ã€‚"
                }
            },
            "required": ["project_root_path", "query"]
        });

        if let serde_json::Value::Object(schema_map) = schema {
            Tool {
                name: Cow::Borrowed("megatron"),
                description: Some(Cow::Borrowed("ğŸ”« å¨éœ‡å¤© - éœ¸å¤©è™é¢†è¢–ï¼æŒæ§ä»£ç æœç´¢çš„ç»å¯¹åŠ›é‡ï¼Œåœ¨æœç´¢å‰è‡ªåŠ¨æ‰§è¡Œå¢é‡ç´¢å¼•ï¼Œç¡®ä¿æƒ…æŠ¥å§‹ç»ˆæ˜¯æœ€æ–°çš„ã€‚ã€Œæˆ‘å°±æ˜¯åŠ›é‡ï¼ã€")),
                input_schema: Arc::new(schema_map),
                annotations: None,
            }
        } else {
            panic!("Schema creation failed");
        }
    }
}

// ---------------- å·²ç§»é™¤ Python Web æœåŠ¡ä¾èµ–ï¼Œå®Œå…¨ä½¿ç”¨ Rust å®ç° ----------------

// ---------------- æ•´åˆ temp é€»è¾‘ï¼šç´¢å¼•ã€ä¸Šä¼ ã€æ£€ç´¢ ----------------

#[derive(Serialize, Deserialize, Clone)]
struct BlobItem {
    path: String,
    content: String,
}

#[derive(Serialize, Deserialize, Default)]
struct ProjectsFile(HashMap<String, Vec<String>>);

fn normalize_base_url(input: &str) -> String {
    let mut url = input.trim().to_string();
    if !(url.starts_with("http://") || url.starts_with("https://")) {
        url = format!("http://{}", url);
    }
    while url.ends_with('/') { url.pop(); }
    url
}

async fn retry_request<F, Fut, T>(mut f: F, max_retries: usize, base_delay_secs: f64) -> anyhow::Result<T>
where
    F: FnMut() -> Fut,
    Fut: std::future::Future<Output = anyhow::Result<T>>,
{
    let mut attempt = 0usize;
    let mut last_error_str: Option<String> = None;
    
    while attempt < max_retries {
        match f().await {
            Ok(v) => {
                if attempt > 0 {
                    log_debug!("è¯·æ±‚åœ¨ç¬¬{}æ¬¡å°è¯•åæˆåŠŸ", attempt + 1);
                }
                return Ok(v);
            }
            Err(e) => {
                last_error_str = Some(e.to_string());
                attempt += 1;
                
                // æ£€æŸ¥æ˜¯å¦ä¸ºå¯é‡è¯•çš„é”™è¯¯
                let error_str = e.to_string();
                let is_retryable = error_str.contains("timeout") 
                    || error_str.contains("connection") 
                    || error_str.contains("network")
                    || error_str.contains("temporary");
                
                if attempt >= max_retries || !is_retryable {
                    log_debug!("è¯·æ±‚å¤±è´¥ï¼Œä¸å†é‡è¯•: {}", e);
                    return Err(e);
                }
                
                let delay = base_delay_secs * 2f64.powi((attempt as i32) - 1);
                let ms = (delay * 1000.0) as u64;
                log_debug!("è¯·æ±‚å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•({}/{}), ç­‰å¾… {}ms: {}", attempt, max_retries, ms, e);
                tokio::time::sleep(Duration::from_millis(ms)).await;
            }
        }
    }
    
    Err(last_error_str
        .and_then(|s| anyhow::anyhow!(s).into())
        .unwrap_or_else(|| anyhow::anyhow!("æœªçŸ¥é”™è¯¯")))
}

fn home_projects_file() -> PathBuf {
    let home = dirs::home_dir().unwrap_or_else(|| PathBuf::from("."));
    let data_dir = home.join(".acemcp").join("data");
    let _ = fs::create_dir_all(&data_dir);
    data_dir.join("projects.json")
}

/// è¯»å–æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒå¤šç§ç¼–ç æ£€æµ‹
/// å°è¯•çš„ç¼–ç é¡ºåºï¼šutf-8, gbk (åŒ…å« gb2312), windows-1252 (åŒ…å« latin-1)
/// å¦‚æœéƒ½å¤±è´¥ï¼Œåˆ™ä½¿ç”¨ utf-8 with errors='ignore'
fn read_file_with_encoding(path: &Path) -> Option<String> {
    let mut file = fs::File::open(path).ok()?;
    let mut buf = Vec::new();
    if file.read_to_end(&mut buf).is_err() {
        return None;
    }

    // å°è¯• utf-8
    let (decoded, _, had_errors) = UTF_8.decode(&buf);
    if !had_errors {
        return Some(decoded.into_owned());
    }

    // å°è¯• gbk
    let (decoded, _, had_errors) = GBK.decode(&buf);
    if !had_errors {
        log_debug!("æˆåŠŸä½¿ç”¨ GBK ç¼–ç è¯»å–æ–‡ä»¶: {:?}", path);
        return Some(decoded.into_owned());
    }

    // å°è¯• gb2312 (GBK æ˜¯ GB2312 çš„è¶…é›†ï¼Œå¯ä»¥å¤„ç† GB2312 ç¼–ç )
    // encoding_rs ä¸­æ²¡æœ‰å•ç‹¬çš„ GB2312ï¼Œä½¿ç”¨ GBK ä»£æ›¿
    // GBK å·²ç»åœ¨ä¸Šä¸€æ­¥å°è¯•è¿‡äº†ï¼Œè¿™é‡Œè·³è¿‡

    // å°è¯• latin-1 (WINDOWS_1252 æ˜¯ ISO-8859-1 çš„è¶…é›†ï¼Œå¯ä»¥å¤„ç†å¤§éƒ¨åˆ† latin-1 ç¼–ç )
    let (decoded, _, had_errors) = WINDOWS_1252.decode(&buf);
    if !had_errors {
        log_debug!("æˆåŠŸä½¿ç”¨ WINDOWS_1252 ç¼–ç è¯»å–æ–‡ä»¶: {:?}", path);
        return Some(decoded.into_owned());
    }

    // å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œä½¿ç”¨ utf-8 with errors='ignore' (lossy è§£ç )
    let (decoded, _, _) = UTF_8.decode(&buf);
    log_debug!("ä½¿ç”¨ UTF-8 (lossy) è¯»å–æ–‡ä»¶ï¼Œéƒ¨åˆ†å­—ç¬¦å¯èƒ½ä¸¢å¤±: {:?}", path);
    Some(decoded.into_owned())
}

fn sha256_hex(path: &str, content: &str) -> String {
    let mut ctx = ShaContext::new(&SHA256);
    // å…ˆæ›´æ–°è·¯å¾„çš„å“ˆå¸Œï¼Œå†æ›´æ–°å†…å®¹çš„å“ˆå¸Œï¼Œä¸Pythonç‰ˆæœ¬ä¿æŒä¸€è‡´
    ctx.update(path.as_bytes());
    ctx.update(content.as_bytes());
    let digest = ctx.finish();
    hex::encode(digest.as_ref())
}

/// åˆ†å‰²æ–‡ä»¶å†…å®¹ä¸ºå¤šä¸ª blobï¼ˆå¦‚æœè¶…è¿‡æœ€å¤§è¡Œæ•°ï¼‰
/// ä¸ Python ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼šchunk ç´¢å¼•ä» 1 å¼€å§‹
fn split_content(path: &str, content: &str, max_lines: usize) -> Vec<BlobItem> {
    let lines: Vec<&str> = content.split_inclusive('\n').collect();
    let total_lines = lines.len();
    
    // å¦‚æœæ–‡ä»¶åœ¨é™åˆ¶å†…ï¼Œè¿”å›å•ä¸ª blob
    if total_lines <= max_lines {
        return vec![BlobItem { path: path.to_string(), content: content.to_string() }];
    }

    // è®¡ç®—éœ€è¦çš„ chunk æ•°é‡
    let num_chunks = (total_lines + max_lines - 1) / max_lines;
    let mut blobs = Vec::new();

    // æŒ‰ chunk ç´¢å¼•åˆ†å‰²ï¼ˆä» 0 å¼€å§‹ï¼Œä½†æ˜¾ç¤ºæ—¶ä» 1 å¼€å§‹ï¼‰
    for chunk_idx in 0..num_chunks {
        let start_line = chunk_idx * max_lines;
        let end_line = usize::min(start_line + max_lines, total_lines);
        let chunk_lines = &lines[start_line..end_line];
        let chunk_content = chunk_lines.join("");

        // chunk ç¼–å·ä» 1 å¼€å§‹ï¼ˆä¸ Python ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
        let chunk_path = format!("{}#chunk{}of{}", path, chunk_idx + 1, num_chunks);
        blobs.push(BlobItem { path: chunk_path, content: chunk_content });
    }

    blobs
}

/// æ„å»ºæ’é™¤æ¨¡å¼çš„ GlobSet
fn build_exclude_globset(exclude_patterns: &[String]) -> Result<GlobSet> {
    let mut builder = GlobSetBuilder::new();
    for pattern in exclude_patterns {
        // å°è¯•å°†æ¨¡å¼è½¬æ¢ä¸º Glob
        if let Ok(glob) = Glob::new(pattern) {
            builder.add(glob);
        } else {
            log_debug!("æ— æ•ˆçš„æ’é™¤æ¨¡å¼ï¼Œè·³è¿‡: {}", pattern);
        }
    }
    builder.build().map_err(|e| anyhow::anyhow!("æ„å»ºæ’é™¤æ¨¡å¼å¤±è´¥: {}", e))
}

/// æ£€æŸ¥è·¯å¾„æ˜¯å¦åº”è¯¥è¢«æ’é™¤
/// ä½¿ç”¨ globset è¿›è¡Œå®Œæ•´çš„ fnmatch æ¨¡å¼åŒ¹é…ï¼ˆä¸ Python ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
/// Python ç‰ˆæœ¬ä½¿ç”¨ fnmatch.fnmatch æ£€æŸ¥è·¯å¾„çš„å„ä¸ªéƒ¨åˆ†å’Œå®Œæ•´è·¯å¾„
fn should_exclude(path: &Path, root: &Path, exclude_globset: Option<&GlobSet>) -> bool {
    if exclude_globset.is_none() {
        return false;
    }
    let globset = exclude_globset.unwrap();

    // è·å–ç›¸å¯¹è·¯å¾„
    let rel = match path.strip_prefix(root) {
        Ok(rel) => rel,
        Err(_) => path,
    };

    // è½¬æ¢ä¸ºä½¿ç”¨æ­£æ–œæ çš„å­—ç¬¦ä¸²ï¼ˆç”¨äºåŒ¹é…ï¼‰
    let rel_forward = rel.to_string_lossy().replace('\\', "/");
    
    // æ£€æŸ¥å®Œæ•´ç›¸å¯¹è·¯å¾„ï¼ˆä¸ Python ç‰ˆæœ¬çš„ fnmatch(path_str, pattern) ä¸€è‡´ï¼‰
    if globset.is_match(&rel_forward) {
        return true;
    }

    // æ£€æŸ¥è·¯å¾„çš„å„ä¸ªéƒ¨åˆ†ï¼ˆä¸ Python ç‰ˆæœ¬çš„ fnmatch(part, pattern) ä¸€è‡´ï¼‰
    for part in rel.iter() {
        if let Some(part_str) = part.to_str() {
            if globset.is_match(part_str) {
                return true;
            }
        }
    }

    false
}

fn build_gitignore(root: &Path) -> Option<Gitignore> {
    let mut builder = GitignoreBuilder::new(root);
    let gi_path = root.join(".gitignore");
    if gi_path.exists() {
        if builder.add(gi_path).is_some() { return None; }
        return match builder.build() { Ok(gi) => Some(gi), Err(_) => None };
    }
    None
}

fn collect_blobs(root: &str, text_exts: &[String], exclude_patterns: &[String], max_lines_per_blob: usize) -> anyhow::Result<Vec<BlobItem>> {
    let root_path = PathBuf::from(root);
    if !root_path.exists() { anyhow::bail!("é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {}", root); }
    
    log_important!(info, "å¼€å§‹æ”¶é›†ä»£ç æ–‡ä»¶: æ ¹ç›®å½•={}, æ‰©å±•å={:?}, æ’é™¤æ¨¡å¼={:?}", root, text_exts, exclude_patterns);
    
    // æ„å»ºæ’é™¤æ¨¡å¼çš„ GlobSet
    let exclude_globset = if exclude_patterns.is_empty() {
        None
    } else {
        match build_exclude_globset(exclude_patterns) {
            Ok(gs) => Some(gs),
            Err(e) => {
                log_debug!("æ„å»ºæ’é™¤æ¨¡å¼å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•åŒ¹é…: {}", e);
                None
            }
        }
    };
    
    let mut out = Vec::new();
    let gitignore = build_gitignore(&root_path);
    let mut dirs_stack = vec![root_path.clone()];
    let mut scanned_files = 0;
    let mut indexed_files = 0;
    let mut excluded_count = 0;
    
    while let Some(dir) = dirs_stack.pop() {
        let entries = match fs::read_dir(&dir) { Ok(e) => e, Err(_) => continue };
        for entry in entries.flatten() {
            let p = entry.path();
            
            // æ£€æŸ¥ .gitignore
            if let Some(gi) = &gitignore {
                if gi.matched_path_or_any_parents(&p, p.is_dir()).is_ignore() { continue; }
            }
            
            // æ£€æŸ¥æ’é™¤æ¨¡å¼
            if p.is_dir() {
                if should_exclude(&p, &root_path, exclude_globset.as_ref()) {
                    excluded_count += 1;
                    continue;
                }
                dirs_stack.push(p);
                continue;
            }
            
            scanned_files += 1;
            if should_exclude(&p, &root_path, exclude_globset.as_ref()) {
                excluded_count += 1;
                log_debug!("æ’é™¤æ–‡ä»¶: {:?}", p);
                continue;
            }
            
            // æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            let ext_ok = p.extension().and_then(|s| s.to_str()).map(|e| {
                let dot = format!(".{}", e).to_lowercase();
                text_exts.iter().any(|te| te.eq_ignore_ascii_case(&dot))
            }).unwrap_or(false);
            if !ext_ok { continue; }
            
            // è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆä½¿ç”¨å¤šç¼–ç æ”¯æŒï¼‰
            let rel = p.strip_prefix(&root_path).unwrap_or(&p).to_string_lossy().replace('\\', "/");
            if let Some(content) = read_file_with_encoding(&p) {
                let parts = split_content(&rel, &content, max_lines_per_blob);
                let blob_count = parts.len();
                indexed_files += 1;
                out.extend(parts);
                log_important!(info, "ç´¢å¼•æ–‡ä»¶: path={}, content_length={}, blobs={}", rel, content.len(), blob_count);
            } else {
                log_debug!("æ— æ³•è¯»å–æ–‡ä»¶: {:?}", p);
            }
        }
    }
    
    log_important!(info, "æ–‡ä»¶æ”¶é›†å®Œæˆ: æ‰«ææ–‡ä»¶æ•°={}, ç´¢å¼•æ–‡ä»¶æ•°={}, ç”Ÿæˆblobsæ•°={}, æ’é™¤æ–‡ä»¶/ç›®å½•æ•°={}", scanned_files, indexed_files, out.len(), excluded_count);
    Ok(out)
}

async fn index_and_search(config: &AcemcpConfig, project_root_path: &str, query: &str) -> anyhow::Result<String> {
    let base_url = config.base_url.clone().ok_or_else(|| anyhow::anyhow!("æœªé…ç½® base_url"))?;
    // ä¸¥æ ¼æ ¡éªŒ base_url
    let has_scheme = base_url.starts_with("http://") || base_url.starts_with("https://");
    let has_host = base_url.trim().len() > "https://".len();
    if !has_scheme || !has_host { anyhow::bail!("æ— æ•ˆçš„ base_urlï¼Œè¯·å¡«å†™å®Œæ•´çš„ http(s)://host[:port] æ ¼å¼"); }
    let token = config.token.clone().ok_or_else(|| anyhow::anyhow!("æœªé…ç½® token"))?;
    let batch_size = config.batch_size.unwrap_or(10) as usize;
    let max_lines = config.max_lines_per_blob.unwrap_or(800) as usize;
    let text_exts = config.text_extensions.clone().unwrap_or_default();
    let exclude_patterns = config.exclude_patterns.clone().unwrap_or_default();

    // æ—¥å¿—ï¼šåŸºç¡€é…ç½®
    log_important!(info,
        "=== å¼€å§‹ç´¢å¼•ä»£ç åº“ ==="
    );
    log_important!(info,
        "Acemcpé…ç½®: base_url={}, batch_size={}, max_lines_per_blob={}, text_extsæ•°é‡={}, exclude_patternsæ•°é‡={}",
        base_url,
        batch_size,
        max_lines,
        text_exts.len(),
        exclude_patterns.len()
    );
    log_important!(info,
        "é¡¹ç›®è·¯å¾„: {}", project_root_path
    );

    // æ”¶é›† blobï¼ˆæ ¹æ®æ‰©å±•åä¸æ’é™¤è§„åˆ™ï¼Œç®€åŒ–ç‰ˆ .gitignore æ”¯æŒï¼‰
    log_important!(info, "å¼€å§‹æ”¶é›†ä»£ç æ–‡ä»¶...");
    let blobs = collect_blobs(project_root_path, &text_exts, &exclude_patterns, max_lines)?;
    if blobs.is_empty() { anyhow::bail!("æœªåœ¨é¡¹ç›®ä¸­æ‰¾åˆ°å¯ç´¢å¼•çš„æ–‡æœ¬æ–‡ä»¶"); }

    // åŠ è½½ projects.json
    let projects_path = home_projects_file();
    let mut projects: ProjectsFile = if projects_path.exists() {
        let data = fs::read_to_string(&projects_path).unwrap_or_default();
        serde_json::from_str(&data).unwrap_or_default()
    } else { ProjectsFile::default() };

    let normalized_root = PathBuf::from(project_root_path).canonicalize().unwrap_or_else(|_| PathBuf::from(project_root_path)).to_string_lossy().replace('\\', "/");
    let existing_blob_names: std::collections::HashSet<String> = projects.0.get(&normalized_root).cloned().unwrap_or_default().into_iter().collect();

    // è®¡ç®—æ‰€æœ‰ blob çš„å“ˆå¸Œå€¼ï¼Œå»ºç«‹å“ˆå¸Œåˆ° blob çš„æ˜ å°„
    let mut blob_hash_map: std::collections::HashMap<String, BlobItem> = std::collections::HashMap::new();
    for blob in &blobs {
        let hash = sha256_hex(&blob.path, &blob.content);
        blob_hash_map.insert(hash.clone(), blob.clone());
    }

    // åˆ†ç¦»å·²å­˜åœ¨å’Œæ–°å¢åŠ çš„ blobï¼ˆä¸ Python ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
    let all_blob_hashes: std::collections::HashSet<String> = blob_hash_map.keys().cloned().collect();
    let existing_hashes: std::collections::HashSet<String> = all_blob_hashes.intersection(&existing_blob_names).cloned().collect();
    let new_hashes: std::collections::HashSet<String> = all_blob_hashes.difference(&existing_blob_names).cloned().collect();

    // éœ€è¦ä¸Šä¼ çš„æ–° blob
    let new_blobs: Vec<BlobItem> = new_hashes.iter().filter_map(|h| blob_hash_map.get(h).cloned()).collect();

    log_important!(info,
        "=== ç´¢å¼•ç»Ÿè®¡ ==="
    );
    log_important!(info,
        "æ”¶é›†åˆ°blobsæ€»æ•°: {}, æ—¢æœ‰blobs: {}, æ–°å¢blobs: {}, éœ€è¦ä¸Šä¼ : {}",
        blobs.len(),
        existing_hashes.len(),
        new_hashes.len(),
        new_blobs.len()
    );

    let client = Client::new();

    // æ‰¹é‡ä¸Šä¼ æ–°å¢ blobs
    let mut uploaded_names: Vec<String> = Vec::new();
    let mut failed_batches: Vec<usize> = Vec::new();
    
    if !new_blobs.is_empty() {
        let total_batches = (new_blobs.len() + batch_size - 1) / batch_size;
        log_important!(info,
            "=== å¼€å§‹æ‰¹é‡ä¸Šä¼ ä»£ç ç´¢å¼• ==="
        );
        log_important!(info,
            "ç›®æ ‡ç«¯ç‚¹: {}/batch-upload, æ€»æ‰¹æ¬¡: {}, æ¯æ‰¹ä¸Šé™: {}, æ€»blobs: {}",
            base_url,
            total_batches,
            batch_size,
            new_blobs.len()
        );
        
        for i in 0..total_batches {
            let start = i * batch_size;
            let end = usize::min(start + batch_size, new_blobs.len());
            let batch = &new_blobs[start..end];
            let url = format!("{}/batch-upload", base_url);
            
            log_important!(info,
                "ä¸Šä¼ æ‰¹æ¬¡ {}/{}: url={}, blobs={}",
                i + 1,
                total_batches,
                url,
                batch.len()
            );
            
            // è¯¦ç»†è®°å½•æ¯ä¸ª blob çš„ä¿¡æ¯
            for (idx, blob) in batch.iter().enumerate() {
                log_important!(info,
                    "  æ‰¹æ¬¡ {} - Blob {}/{}: path={}, content_length={}",
                    i + 1,
                    idx + 1,
                    batch.len(),
                    blob.path,
                    blob.content.len()
                );
            }
            
            let payload = serde_json::json!({"blobs": batch});
            log_important!(info, "æ‰¹æ¬¡è½½è·å¤§å°: {} å­—èŠ‚", payload.to_string().len());
            
            match retry_request(|| async {
                let r = client
                    .post(&url)
                    .header(AUTHORIZATION, format!("Bearer {}", token))
                    .header(CONTENT_TYPE, "application/json")
                    .json(&payload)
                    .send()
                    .await?;
                
                let status = r.status();
                log_important!(info, "HTTPå“åº”çŠ¶æ€: {}", status);
                
                if !status.is_success() {
                    let body = r.text().await.unwrap_or_default();
                    anyhow::bail!("HTTP {} {}", status, body);
                }
                
                let v: serde_json::Value = r.json().await?;
                log_important!(info, "å“åº”æ•°æ®: {}", serde_json::to_string_pretty(&v).unwrap_or_default());
                Ok(v)
            }, 3, 1.0).await {
                Ok(value) => {
                    if let Some(arr) = value.get("blob_names").and_then(|v| v.as_array()) {
                        let mut batch_names: Vec<String> = Vec::new();
                        for v in arr { 
                            if let Some(s) = v.as_str() { 
                                batch_names.push(s.to_string()); 
                            }
                        }
                        
                        if batch_names.is_empty() {
                            log_important!(info, "æ‰¹æ¬¡ {} è¿”å›äº†ç©ºçš„blobåç§°åˆ—è¡¨", i + 1);
                            failed_batches.push(i + 1);
                        } else {
                            uploaded_names.extend(batch_names.clone());
                            log_important!(info, "æ‰¹æ¬¡ {} ä¸Šä¼ æˆåŠŸï¼Œè·å¾— {} ä¸ªblobåç§°", i + 1, batch_names.len());
                            // è¯¦ç»†è®°å½•æ¯ä¸ªä¸Šä¼ æˆåŠŸçš„ blob åç§°
                            for (idx, name) in batch_names.iter().enumerate() {
                                log_important!(info, "  æ‰¹æ¬¡ {} - ä¸Šä¼ æˆåŠŸ Blob {}/{}: name={}", i + 1, idx + 1, batch_names.len(), name);
                            }
                        }
                    } else {
                        log_important!(info, "æ‰¹æ¬¡ {} å“åº”ä¸­ç¼ºå°‘blob_nameså­—æ®µ", i + 1);
                        failed_batches.push(i + 1);
                    }
                }
                Err(e) => {
                    log_important!(info, "æ‰¹æ¬¡ {} ä¸Šä¼ å¤±è´¥: {}", i + 1, e);
                    failed_batches.push(i + 1);
                }
            }
        }
        
        // ä¸Šä¼ ç»“æœæ€»ç»“
        log_important!(info,
            "=== ä¸Šä¼ ç»“æœæ€»ç»“ ==="
        );
        if !failed_batches.is_empty() {
            log_important!(info, "ä¸Šä¼ å®Œæˆï¼Œä½†æœ‰å¤±è´¥çš„æ‰¹æ¬¡: {:?}, æˆåŠŸä¸Šä¼ blobs: {}", failed_batches, uploaded_names.len());
        } else {
            log_important!(info, "æ‰€æœ‰æ‰¹æ¬¡ä¸Šä¼ æˆåŠŸï¼Œå…±ä¸Šä¼  {} ä¸ªblobs", uploaded_names.len());
        }
    } else {
        log_important!(info, "æ²¡æœ‰æ–°çš„blobéœ€è¦ä¸Šä¼ ï¼Œä½¿ç”¨å·²æœ‰ç´¢å¼•");
    }

    // åˆå¹¶å¹¶ä¿å­˜ projects.jsonï¼ˆä¸ Python ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
    // åªä¿ç•™å½“å‰é¡¹ç›®ä¸­ä»ç„¶å­˜åœ¨çš„ blob çš„å“ˆå¸Œå€¼ï¼ˆè‡ªåŠ¨åˆ é™¤å·²åˆ é™¤çš„ blobï¼‰
    let all_blob_names: Vec<String> = existing_hashes.into_iter().chain(uploaded_names.into_iter()).collect();
    projects.0.insert(normalized_root.clone(), all_blob_names.clone());
    if let Ok(s) = serde_json::to_string_pretty(&projects) { let _ = fs::write(projects_path, s); }

    // ä½¿ç”¨åˆå¹¶åçš„ blob_namesï¼ˆä¸ Python ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰
    let blob_names = all_blob_names;
    if blob_names.is_empty() { 
        log_important!(info, "ç´¢å¼•åæœªæ‰¾åˆ° blobsï¼Œé¡¹ç›®è·¯å¾„: {}", normalized_root);
        anyhow::bail!("ç´¢å¼•åæœªæ‰¾åˆ° blobs"); 
    }

    // å‘èµ·æ£€ç´¢
    log_important!(info,
        "=== å¼€å§‹ä»£ç æ£€ç´¢ ==="
    );
    let search_url = format!("{}/agents/codebase-retrieval", base_url);
    log_important!(info, "æ£€ç´¢è¯·æ±‚: url={}, ä½¿ç”¨blobsæ•°é‡={}, æŸ¥è¯¢å†…å®¹={}", search_url, blob_names.len(), query);
    
    let payload = serde_json::json!({
        "information_request": query,
        "blobs": {"checkpoint_id": serde_json::Value::Null, "added_blobs": blob_names, "deleted_blobs": []},
        "dialog": [],
        "max_output_length": 0,
        "disable_codebase_retrieval": false,
        "enable_commit_retrieval": false,
    });
    
    log_important!(info, "æ£€ç´¢è½½è·å¤§å°: {} å­—èŠ‚", payload.to_string().len());
    
    let value: serde_json::Value = retry_request(|| async {
        let r = client
            .post(&search_url)
            .header(AUTHORIZATION, format!("Bearer {}", token))
            .header(CONTENT_TYPE, "application/json")
            .json(&payload)
            .send()
            .await?;
        
        let status = r.status();
        log_important!(info, "æ£€ç´¢è¯·æ±‚HTTPå“åº”çŠ¶æ€: {}", status);
        
        if !status.is_success() {
            let body = r.text().await.unwrap_or_default();
            anyhow::bail!("HTTP {} {}", status, body);
        }
        
        let v: serde_json::Value = r.json().await?;
        log_important!(info, "æ£€ç´¢å“åº”æ•°æ®: {}", serde_json::to_string_pretty(&v).unwrap_or_default());
        Ok(v)
    }, 3, 2.0).await?;
    
    let text = value
        .get("formatted_retrieval")
        .and_then(|v| v.as_str())
        .unwrap_or("")
        .to_string();
        
    if text.is_empty() { 
        log_important!(info, "æœç´¢è¿”å›ç©ºç»“æœ");
        Ok("No relevant code context found for your query.".to_string()) 
    } else { 
        log_important!(info, "æœç´¢æˆåŠŸï¼Œè¿”å›æ–‡æœ¬é•¿åº¦: {}", text.len());
        Ok(text) 
    }
}
